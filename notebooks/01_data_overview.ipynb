{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's Define the root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('../StudentProjectData/TrainData3.txt'),\n",
       " PosixPath('../StudentProjectData/TrainData2.txt'),\n",
       " PosixPath('../StudentProjectData/TrainLabel4.txt'),\n",
       " PosixPath('../StudentProjectData/TrainData1.txt'),\n",
       " PosixPath('../StudentProjectData/TestData4.txt'),\n",
       " PosixPath('../StudentProjectData/TrainLabel3.txt'),\n",
       " PosixPath('../StudentProjectData/.DS_Store'),\n",
       " PosixPath('../StudentProjectData/TrainData4.txt'),\n",
       " PosixPath('../StudentProjectData/TestData1.txt'),\n",
       " PosixPath('../StudentProjectData/TrainLabel2.txt'),\n",
       " PosixPath('../StudentProjectData/TestData3.txt'),\n",
       " PosixPath('../StudentProjectData/TestData2.txt'),\n",
       " PosixPath('../StudentProjectData/TrainLabel1.txt'),\n",
       " PosixPath('../StudentProjectData/MissingData2.txt'),\n",
       " PosixPath('../StudentProjectData/MissingData1.txt')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ROOT = Path(\"..\")\n",
    "DATA = ROOT / \"StudentProjectData\"\n",
    "list(DATA.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loaders to explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_txt_matrix(path):\n",
    "    text = open(path).read()\n",
    "\n",
    "    if \",\" in text.splitlines()[0]:   # if the first line contains commas\n",
    "        return np.loadtxt(path, delimiter=\",\")\n",
    "    else:\n",
    "        return np.loadtxt(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Classification sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_classification(i):\n",
    "    X_train = load_txt_matrix(DATA / f\"TrainData{i}.txt\")\n",
    "    y_train = load_txt_matrix(DATA / f\"TrainLabel{i}.txt\")\n",
    "    X_test = load_txt_matrix(DATA / f\"TestData{i}.txt\" )\n",
    "    return X_train, y_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Missing Value Data Sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_missing(i):\n",
    "    return load_txt_matrix(DATA / f\"MissingData{i}.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect Classification Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "––––––––––––––––––––––––––––––––––––––––\n",
      "Dataset 1\n",
      "Train shape:  (150, 3312)\n",
      "Label shape:  (150,)\n",
      "Test shape:  (53, 3312)\n",
      "––––––––––––––––––––––––––––––––––––––––\n",
      "Dataset 2\n",
      "Train shape:  (100, 9182)\n",
      "Label shape:  (100,)\n",
      "Test shape:  (74, 9182)\n",
      "––––––––––––––––––––––––––––––––––––––––\n",
      "Dataset 3\n",
      "Train shape:  (6300, 13)\n",
      "Label shape:  (6300,)\n",
      "Test shape:  (2693, 13)\n",
      "––––––––––––––––––––––––––––––––––––––––\n",
      "Dataset 4\n",
      "Train shape:  (2547, 112)\n",
      "Label shape:  (2547,)\n",
      "Test shape:  (1092, 112)\n",
      "––––––––––––––––––––––––––––––––––––––––\n"
     ]
    }
   ],
   "source": [
    "print(\"–\"*40)\n",
    "for i in range(1,5):\n",
    "    X_train, y_train, X_test = load_classification(i)\n",
    "    print(f\"Dataset {i}\")\n",
    "    print(\"Train shape: \", X_train.shape)\n",
    "    print(\"Label shape: \", y_train.shape)\n",
    "    print(\"Test shape: \", X_test.shape)\n",
    "    print(\"–\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify Missing Values: \n",
    "- Missing values are encoded as 1e99."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1: missing train = 9936, missing_test = 7021\n",
      "Dataset 2: missing train = 0, missing_test = 0\n",
      "Dataset 3: missing train = 1886, missing_test = 0\n",
      "Dataset 4: missing train = 0, missing_test = 0\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,5):\n",
    "    X_train, y_train, X_test = load_classification(i)\n",
    "    missing_train = np.sum(X_train == 1e99)\n",
    "    missing_test = np.sum(X_test == 1e99)\n",
    "    print(f\"Dataset {i}: missing train = {missing_train}, missing_test = {missing_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For missing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MissingData1: 118 missing (3.48%)\n",
      "MissingData2: 3762 missing (9.93%)\n"
     ]
    }
   ],
   "source": [
    "for i in [1,2]:\n",
    "    M = load_missing(i)\n",
    "    missing = np.sum(M == 1e99)\n",
    "    total = M.size \n",
    "    pct = missing / total * 100\n",
    "    print(f\"MissingData{i}: {missing} missing ({pct:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic statistics (feature-level):\n",
    "- We will use masked arrays so missing values don’t distort stats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_stats(X):\n",
    "    m = np.ma.masked_where(X == 1e99, X)\n",
    "    return m.mean(), m.std(), m.min(), m.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1 | mean=2.19, std=0.56, range=(1.00, 3.97)\n",
      "Dataset 2 | mean=2.17, std=0.61, range=(1.30, 4.83)\n",
      "Dataset 3 | mean=2.74, std=2.00, range=(0.00, 9.00)\n",
      "Dataset 4 | mean=0.26, std=123.60, range=(-359.12, 356.42)\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,5):\n",
    "    X_train, y_train, X_test = load_classification(i)\n",
    "    mean, std, mn, mx = masked_stats(X_train)\n",
    "    print(f\"Dataset {i} | mean={mean:.2f}, std={std:.2f}, range=({mn:.2f}, {mx:.2f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class distribution:\n",
    "- check whether the datasets are imbalanced or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1:\n",
      "    class 1.0: 108 samples\n",
      "    class 2.0: 14 samples\n",
      "    class 3.0: 11 samples\n",
      "    class 4.0: 14 samples\n",
      "    class 5.0: 3 samples\n",
      "\n",
      "Dataset 2:\n",
      "    class 1.0: 10 samples\n",
      "    class 2.0: 8 samples\n",
      "    class 3.0: 12 samples\n",
      "    class 4.0: 11 samples\n",
      "    class 5.0: 11 samples\n",
      "    class 6.0: 10 samples\n",
      "    class 7.0: 6 samples\n",
      "    class 8.0: 9 samples\n",
      "    class 9.0: 6 samples\n",
      "    class 10.0: 9 samples\n",
      "    class 11.0: 8 samples\n",
      "\n",
      "Dataset 3:\n",
      "    class 1.0: 1235 samples\n",
      "    class 2.0: 554 samples\n",
      "    class 3.0: 488 samples\n",
      "    class 4.0: 566 samples\n",
      "    class 5.0: 495 samples\n",
      "    class 6.0: 777 samples\n",
      "    class 7.0: 677 samples\n",
      "    class 8.0: 912 samples\n",
      "    class 9.0: 596 samples\n",
      "\n",
      "Dataset 4:\n",
      "    class 1.0: 288 samples\n",
      "    class 2.0: 275 samples\n",
      "    class 3.0: 270 samples\n",
      "    class 4.0: 292 samples\n",
      "    class 5.0: 278 samples\n",
      "    class 6.0: 287 samples\n",
      "    class 7.0: 289 samples\n",
      "    class 8.0: 298 samples\n",
      "    class 9.0: 270 samples\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,5):\n",
    "    _, y, _ = load_classification(i)\n",
    "    uniques, counts = np.unique(y, return_counts=True)\n",
    "    print(f\"Dataset {i}:\")\n",
    "    for u, c in zip(uniques, counts):\n",
    "        print(f\"    class {u}: {c} samples\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset 1:\n",
    "\n",
    "Extremely imbalanced: 108 samples in class 1, only 3 samples in class 5.\n",
    "\n",
    "High percentage of missing values: 9,936 missing in Train and 7,021 in Test.\n",
    "\n",
    "Mean and standard deviation values suggest feature scaling is required.\n",
    "\n",
    "Conclusion: \n",
    "- Prioritize robust imputation and feature scaling. Consider weighted classification or using KNN with careful tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset 2:\n",
    "\n",
    "Large feature set: 9,182 features with no missing values.\n",
    "Classes are more balanced compared to Dataset 1.\n",
    "\n",
    "Conclusion: \n",
    "- Use dimensionality reduction or simple classification methods to avoid overfitting and computational issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset 3:\n",
    "\n",
    "Moderate missing values in Train (1,886 missing)\n",
    "13 features only.\n",
    "\n",
    "Healthy class distribution.\n",
    "\n",
    "Conclusion: \n",
    "- Straightforward imputation and classification can be applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset 4\n",
    "\n",
    "No missing values, but feature distribution is problematic: very large standard deviation caused by outliers or variation in the data.\n",
    "\n",
    "Classes are fairly balanced.\n",
    "\n",
    "Conclusion: \n",
    "- Strong normalization or feature scaling is necessary.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
